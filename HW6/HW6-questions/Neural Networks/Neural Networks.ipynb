{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "489b9bb3",
   "metadata": {},
   "source": [
    "<div align=center>\n",
    "\t\t\n",
    "<p></p>\n",
    "<p></p>\n",
    "<font size=5>\n",
    "In the Name of God\n",
    "<font/>\n",
    "<p></p>\n",
    " <br/>\n",
    "    <br/>\n",
    "    <br/>\n",
    "<font color=#FF7500>\n",
    "Sharif University of Technology - Departmenet of Computer Engineering\n",
    "</font>\n",
    "<p></p>\n",
    "<font color=blue>\n",
    "Artifical Intelligence - Dr. Mohammad Hossein Rohban\n",
    "</font>\n",
    "<br/>\n",
    "<br/>\n",
    "Spring 2022\n",
    "\n",
    "</div>\n",
    "\n",
    "<hr/>\n",
    "\t\t<div align=center>\n",
    "\t\t    <font color=red size=6>\n",
    "\t\t\t    <br />\n",
    "Practical Assignment 6 Neural Networks\n",
    "            \t<br/>\n",
    "\t\t\t</font>\n",
    "    <br/>\n",
    "    <br/>\n",
    "<font size=4>\n",
    "                <br/><b>\n",
    "              Cheating is Strongly Prohibited\n",
    "                </b><br/><br/>\n",
    "                <font color=red>\n",
    "Please run all the cells.\n",
    "     </font>\n",
    "</font>\n",
    "                <br/>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c2fd6d",
   "metadata": {},
   "source": [
    "# Personal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e90a030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your student number\n",
    "student_number = \n",
    "Name = ''\n",
    "Last_Name = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339da203",
   "metadata": {},
   "source": [
    "# Rules\n",
    "- You are not allowed to add or remove cells. You **must use the provided space to write your code**. If you don't follow this rule, **your Practical Assignment won't be graded**. \n",
    "- You **are** allowed to use **for loops** only in the implementation of the **FullyConnectedNet** class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881f2e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install future\n",
    "!pip install pandas\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efb6140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Helper_codes.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from Helper_codes.MNIST_data import get_MNIST_data, get_normalized_MNIST_data\n",
    "from builtins import range\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from Helper_codes.solver import *\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from Helper_codes.california_housing import *\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def print_mean_std(x,axis=0):\n",
    "    print(f\"  means: {x.mean(axis=axis)}\")\n",
    "    print(f\"  stds:  {x.std(axis=axis)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c3af9e",
   "metadata": {},
   "source": [
    "# Fully-Connected Neural Nets\n",
    "In this exercise we will implement fully-connected networks using a modular approach. For each layer we will implement a `forward` and a `backward` function. The `forward` function will receive inputs, weights, and other parameters and will return both an output and a `cache` object storing data needed for the backward pass, like this:\n",
    "\n",
    "```python\n",
    "def layer_forward(x, w):\n",
    "  \"\"\" Receive inputs x and weights w \"\"\"\n",
    "  # Do some computations ...\n",
    "  z = # ... some intermediate value\n",
    "  # Do some more computations ...\n",
    "  out = # the output\n",
    "   \n",
    "  cache = (x, w, z, out) # Values we need to compute gradients\n",
    "   \n",
    "  return out, cache\n",
    "```\n",
    "\n",
    "The backward pass will receive upstream derivatives and the `cache` object, and will return gradients with respect to the inputs and weights, like this:\n",
    "\n",
    "```python\n",
    "def layer_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Receive dout (derivative of loss with respect to outputs) and cache,\n",
    "  and compute derivative with respect to inputs.\n",
    "  \"\"\"\n",
    "  # Unpack cache values\n",
    "  x, w, z, out = cache\n",
    "  \n",
    "  # Use values in cache to compute derivatives\n",
    "  dx = # Derivative of loss with respect to x\n",
    "  dw = # Derivative of loss with respect to w\n",
    "  \n",
    "  return dx, dw\n",
    "```\n",
    "\n",
    "After implementing a bunch of layers this way, we will be able to easily combine them to build classifiers with different architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649895ab",
   "metadata": {},
   "source": [
    "# Affine layer: forward\n",
    "Implement the `affine_forward` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c584b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for an affine (fully-connected) layer.\n",
    "\n",
    "    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
    "    examples, where each example x[i] has shape (d_1, ..., d_k). We will\n",
    "    reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
    "    then transform it to an output vector of dimension M.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
    "    - w: A numpy array of weights, of shape (D, M)\n",
    "    - b: A numpy array of biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: output, of shape (N, M)\n",
    "    - cache: (x, w, b)\n",
    "    \"\"\"\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the affine forward pass. Store the result in out. You   #\n",
    "    # will need to reshape the input into rows.                               #\n",
    "    ###########################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdd5827",
   "metadata": {},
   "source": [
    "You can test your implementaion by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80db2fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the affine_forward function\n",
    "\n",
    "num_inputs = 2\n",
    "input_shape = (4, 5, 6)\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "out, _ = affine_forward(x, w, b)\n",
    "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "# Compare your output with ours. The error should be around e-9 or less.\n",
    "print('Testing affine_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e64b312",
   "metadata": {},
   "source": [
    "# Affine layer: backward\n",
    "Now implement the `affine_backward` function and test your implementation using numeric gradient checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61d5b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for an affine layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivative, of shape (N, M)\n",
    "    - cache: Tuple of:\n",
    "      - x: Input data, of shape (N, d_1, ... d_k)\n",
    "      - w: Weights, of shape (D, M)\n",
    "      - b: Biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n",
    "    - dw: Gradient with respect to w, of shape (D, M)\n",
    "    - db: Gradient with respect to b, of shape (M,)\n",
    "    \"\"\"    \n",
    "    ###########################################################################\n",
    "    # TODO: Implement the affine backward pass.                               #\n",
    "    ###########################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64de3af3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test the affine_backward function\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "_, cache = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, cache)\n",
    "\n",
    "# The error should be around e-10 or less\n",
    "print('Testing affine_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5910d2",
   "metadata": {},
   "source": [
    "# ReLU activation: forward\n",
    "Implement the forward pass for the ReLU activation function in the `relu_forward` function and test your implementation using numeric gradient checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c537098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_forward(x):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - x: Inputs, of any shape\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output, of the same shape as x\n",
    "    - cache: x\n",
    "    \"\"\"\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the ReLU forward pass.                                  #\n",
    "    ###########################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc17aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the relu_forward function\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out, _ = relu_forward(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "# Compare your output with ours. The error should be on the order of e-8\n",
    "print('Testing relu_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6171f37c",
   "metadata": {},
   "source": [
    "# ReLU activation: backward\n",
    "Now implement the backward pass for the ReLU activation function in the `relu_backward` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3187da88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - dout: Upstream derivatives, of any shape\n",
    "    - cache: Input x, of same shape as dout\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x\n",
    "    \"\"\"\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the ReLU backward pass.                                 #\n",
    "    ###########################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88966dd6",
   "metadata": {},
   "source": [
    "You can test your implementation using numeric gradient checking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81e30fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the relu_backward function\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "# The error should be on the order of e-12\n",
    "print('Testing relu_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f2e4d2",
   "metadata": {},
   "source": [
    "# Sigmoid activation: forward\n",
    "Implement the forward pass for the Sigmoid activation function in the `sigmoid_forward` function and test your implementation using numeric gradient checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872854b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_forward(x):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for a layer of Sigmoid.\n",
    "\n",
    "    Input:\n",
    "    - x: Inputs, of any shape\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output, of the same shape as x\n",
    "    - cache: x\n",
    "    \"\"\"\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the Sigmoid forward pass.                               #\n",
    "    ###########################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aadfec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the sigmoid_forward function\n",
    "\n",
    "x = np.linspace(-6, 6, num=12).reshape(3, 4)\n",
    "\n",
    "out, _ = sigmoid_forward(x)\n",
    "correct_out = np.array([[0.00247262, 0.00732514, 0.0214955 , 0.06138311],\n",
    "                        [0.16296047, 0.36691963, 0.63308037, 0.83703953],\n",
    "                        [0.93861689, 0.9785045 , 0.99267486, 0.99752738]])\n",
    "\n",
    "# Compare your output with ours. The error should be on the order of e-7\n",
    "print('Testing sigmoid_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa4e6a8",
   "metadata": {},
   "source": [
    "# Sigmoid activation: backward\n",
    "Now implement the backward pass for the Sigmoid activation function in the `sigmoid_backward` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe527984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a layer of Sigmoid.\n",
    "\n",
    "    Input:\n",
    "    - dout: Upstream derivatives, of any shape\n",
    "    - cache: Input x, of same shape as dout\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x\n",
    "    \"\"\"\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the Sigmoid backward pass.                              #\n",
    "    ###########################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ec2570",
   "metadata": {},
   "source": [
    "You can test your implementation using numeric gradient checking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06a7eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the sigmoid_backward function\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: sigmoid_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = sigmoid_forward(x)\n",
    "dx = sigmoid_backward(dout, cache)\n",
    "\n",
    "# The error should be on the order of e-11\n",
    "print('Testing sigmoid_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb8d7a7",
   "metadata": {},
   "source": [
    "# \"Sandwich\" layers\n",
    "There are some common patterns of layers that are frequently used in neural nets. For example, affine layers are frequently followed by a ReLU nonlinearity. Implement the forward and backward pass for the affine layer followed by a ReLU nonlinearity in the `affine_relu_forward` and `affine_relu_backward` functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7283bd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_relu_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Convenience layer that performs an affine transform followed by a ReLU\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input to the affine layer\n",
    "    - w, b: Weights for the affine layer\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output from the ReLU\n",
    "    - cache: Object to give to the backward pass\n",
    "    \"\"\"\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the affine-RELU forward pass.                           #\n",
    "    ###########################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e27852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for the affine-relu convenience layer\n",
    "    \n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives, of any shape\n",
    "    - cache: (fc_cache, relu_cache)\n",
    "      \n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x\n",
    "    - dw: Gradient with respect to w\n",
    "    - db: Gradient with respect to b\n",
    "    \"\"\"\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the affine-RELU backward pass.                          #\n",
    "    ###########################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdd0b18",
   "metadata": {},
   "source": [
    "You can test your implementation using numeric gradient checking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53fb1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the affine_relu_backward function\n",
    "\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(2, 3, 4)\n",
    "w = np.random.randn(12, 10)\n",
    "b = np.random.randn(10)\n",
    "dout = np.random.randn(2, 10)\n",
    "\n",
    "out, cache = affine_relu_forward(x, w, b)\n",
    "dx, dw, db = affine_relu_backward(dout, cache)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_relu_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_relu_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_relu_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "# Relative error should be around e-10 or less\n",
    "print('Testing affine_relu_forward and affine_relu_backward:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd6b07f",
   "metadata": {},
   "source": [
    "# Batch Normalization: Forward Pass\n",
    "Implement the batch normalization forward pass in the function `batchnorm_forward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae14e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnorm_forward(x, gamma, beta, bn_param):\n",
    "    \"\"\"Forward pass for batch normalization.\n",
    "\n",
    "    During training the sample mean and (uncorrected) sample variance are\n",
    "    computed from minibatch statistics and used to normalize the incoming data.\n",
    "    During training we also keep an exponentially decaying running mean of the\n",
    "    mean and variance of each feature, and these averages are used to normalize\n",
    "    data at test-time.\n",
    "\n",
    "    At each timestep we update the running averages for mean and variance using\n",
    "    an exponential decay based on the momentum parameter:\n",
    "\n",
    "    running_mean = momentum * running_mean + (1 - momentum) * sample_mean\n",
    "    running_var = momentum * running_var + (1 - momentum) * sample_var\n",
    "\n",
    "    Input:\n",
    "    - x: Data of shape (N, D)\n",
    "    - gamma: Scale parameter of shape (D,)\n",
    "    - beta: Shift paremeter of shape (D,)\n",
    "    - bn_param: Dictionary with the following keys:\n",
    "      - mode: 'train' or 'test'; required\n",
    "      - eps: Constant for numeric stability\n",
    "      - momentum: Constant for running mean / variance.\n",
    "      - running_mean: Array of shape (D,) giving running mean of features\n",
    "      - running_var Array of shape (D,) giving running variance of features\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: of shape (N, D)\n",
    "    - cache: A tuple of values needed in the backward pass\n",
    "    \"\"\"\n",
    "    mode = bn_param[\"mode\"]\n",
    "    eps = bn_param.get(\"eps\", 1e-5)\n",
    "    momentum = bn_param.get(\"momentum\", 0.9)\n",
    "\n",
    "    N, D = x.shape\n",
    "    running_mean = bn_param.get(\"running_mean\", np.zeros(D, dtype=x.dtype))\n",
    "    running_var = bn_param.get(\"running_var\", np.zeros(D, dtype=x.dtype))\n",
    "\n",
    "    out, cache = None, None\n",
    "    if mode == \"train\":\n",
    "        #######################################################################\n",
    "        # TODO: Implement the training-time forward pass for batch norm.      #\n",
    "        # Use minibatch statistics to compute the mean and variance, use      #\n",
    "        # these statistics to normalize the incoming data, and scale and      #\n",
    "        # shift the normalized data using gamma and beta.                     #\n",
    "        #                                                                     #\n",
    "        # You should store the output in the variable out. Any intermediates  #\n",
    "        # that you need for the backward pass should be stored in the cache   #\n",
    "        # variable.                                                           #\n",
    "        #                                                                     #\n",
    "        # You should also use your computed sample mean and variance together #\n",
    "        # with the momentum variable to update the running mean and running   #\n",
    "        # variance, storing your result in the running_mean and running_var   #\n",
    "        # variables.                                                          #\n",
    "        #                                                                     #\n",
    "        # Note that though you should be keeping track of the running         #\n",
    "        # variance, you should normalize the data based on the standard       #\n",
    "        # deviation (square root of variance) instead!                        #\n",
    "        #######################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        pass\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        #######################################################################\n",
    "        #                           END OF YOUR CODE                          #\n",
    "        #######################################################################\n",
    "    elif mode == \"test\":\n",
    "        #######################################################################\n",
    "        # TODO: Implement the test-time forward pass for batch normalization. #\n",
    "        # Use the running mean and variance to normalize the incoming data,   #\n",
    "        # then scale and shift the normalized data using gamma and beta.      #\n",
    "        # Store the result in the out variable.                               #\n",
    "        #######################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        pass\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        #######################################################################\n",
    "        #                          END OF YOUR CODE                           #\n",
    "        #######################################################################\n",
    "    else:\n",
    "        raise ValueError('Invalid forward batchnorm mode \"%s\"' % mode)\n",
    "\n",
    "    # Store the updated running means back into bn_param\n",
    "    bn_param[\"running_mean\"] = running_mean\n",
    "    bn_param[\"running_var\"] = running_var\n",
    "\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b94ccd",
   "metadata": {},
   "source": [
    "Run the following to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71831d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the training-time forward pass by checking means and variances\n",
    "# of features both before and after batch normalization   \n",
    "\n",
    "# Simulate the forward pass for a two-layer network.\n",
    "np.random.seed(231)\n",
    "N, D1, D2, D3 = 200, 50, 60, 3\n",
    "X = np.random.randn(N, D1)\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "\n",
    "print('Before batch normalization:')\n",
    "print_mean_std(a,axis=0)\n",
    "\n",
    "gamma = np.ones((D3,))\n",
    "beta = np.zeros((D3,))\n",
    "\n",
    "# Means should be close to zero and stds close to one.\n",
    "print('After batch normalization (gamma=1, beta=0)')\n",
    "a_norm, _ = batchnorm_forward(a, gamma, beta, {'mode': 'train'})\n",
    "print_mean_std(a_norm,axis=0)\n",
    "\n",
    "gamma = np.asarray([1.0, 2.0, 3.0])\n",
    "beta = np.asarray([11.0, 12.0, 13.0])\n",
    "\n",
    "# Now means should be close to beta and stds close to gamma.\n",
    "print('After batch normalization (gamma=', gamma, ', beta=', beta, ')')\n",
    "a_norm, _ = batchnorm_forward(a, gamma, beta, {'mode': 'train'})\n",
    "print_mean_std(a_norm,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89dca02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the test-time forward pass by running the training-time\n",
    "# forward pass many times to warm up the running averages, and then\n",
    "# checking the means and variances of activations after a test-time\n",
    "# forward pass.\n",
    "\n",
    "np.random.seed(231)\n",
    "N, D1, D2, D3 = 200, 50, 60, 3\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "gamma = np.ones(D3)\n",
    "beta = np.zeros(D3)\n",
    "\n",
    "for t in range(50):\n",
    "    X = np.random.randn(N, D1)\n",
    "    a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "    batchnorm_forward(a, gamma, beta, bn_param)\n",
    "\n",
    "bn_param['mode'] = 'test'\n",
    "X = np.random.randn(N, D1)\n",
    "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "a_norm, _ = batchnorm_forward(a, gamma, beta, bn_param)\n",
    "\n",
    "# Means should be close to zero and stds close to one, but will be\n",
    "# noisier than training-time forward passes.\n",
    "print('After batch normalization (test-time):')\n",
    "print_mean_std(a_norm,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07719acb",
   "metadata": {},
   "source": [
    "# Batch Normalization: Backward Pass\n",
    "Now implement the backward pass for batch normalization in the function `batchnorm_backward`.\n",
    "\n",
    "In the forward pass, given a set of inputs $X=\\begin{bmatrix}x_1\\\\x_2\\\\...\\\\x_N\\end{bmatrix}$, \n",
    "\n",
    "we first calculate the mean $\\mu$ and variance $var$.\n",
    "With $\\mu$ and $var$ calculated, we can calculate the standard deviation $\\sigma$  and normalized data $Y$.\n",
    "The equations and graph illustration below describe the computation ($y_i$ is the i-th element of the vector $Y$).\n",
    "\n",
    "\\begin{align}\n",
    "& \\mu=\\frac{1}{N}\\sum_{k=1}^N x_k  &  var=\\frac{1}{N}\\sum_{k=1}^N (x_k-\\mu)^2 \\\\\n",
    "& \\sigma=\\sqrt{v+\\epsilon}         &  y_i=\\frac{x_i-\\mu}{\\sigma}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06afd7ac",
   "metadata": {},
   "source": [
    "<img src=\"imgs/batchnorm_graph.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462af44e",
   "metadata": {},
   "source": [
    "You should make sure each of the intermediary gradient derivations are all as simplified as possible, for ease of implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b2bf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnorm_backward(dout, cache):\n",
    "    \"\"\"Backward pass for batch normalization.\n",
    "\n",
    "    For this implementation, you should write out a computation graph for\n",
    "    batch normalization on paper and propagate gradients backward through\n",
    "    intermediate nodes.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives, of shape (N, D)\n",
    "    - cache: Variable of intermediates from batchnorm_forward.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to inputs x, of shape (N, D)\n",
    "    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)\n",
    "    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)\n",
    "    \"\"\"\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the backward pass for batch normalization. Store the    #\n",
    "    # results in the dx, dgamma, and dbeta variables.                         #\n",
    "    ###########################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6891d0a",
   "metadata": {},
   "source": [
    "Run the following to numerically check your backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e615e6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient check batchnorm backward pass.\n",
    "np.random.seed(231)\n",
    "N, D = 4, 5\n",
    "x = 5 * np.random.randn(N, D) + 12\n",
    "gamma = np.random.randn(D)\n",
    "beta = np.random.randn(D)\n",
    "dout = np.random.randn(N, D)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "fx = lambda x: batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "fg = lambda a: batchnorm_forward(x, a, beta, bn_param)[0]\n",
    "fb = lambda b: batchnorm_forward(x, gamma, b, bn_param)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "da_num = eval_numerical_gradient_array(fg, gamma.copy(), dout)\n",
    "db_num = eval_numerical_gradient_array(fb, beta.copy(), dout)\n",
    "\n",
    "_, cache = batchnorm_forward(x, gamma, beta, bn_param)\n",
    "dx, dgamma, dbeta = batchnorm_backward(dout, cache)\n",
    "\n",
    "# You should expect to see relative errors between 1e-13 and 1e-8.\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dgamma error: ', rel_error(da_num, dgamma))\n",
    "print('dbeta error: ', rel_error(db_num, dbeta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9227f7e",
   "metadata": {},
   "source": [
    "# Loss layer: Softmax\n",
    "Now implement the loss and gradient for softmax in the `softmax_loss` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99a83c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(x, y):\n",
    "    \"\"\"\n",
    "    Computes the loss and gradient for softmax classification.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth\n",
    "      class for the ith input.\n",
    "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
    "      0 <= y[i] < C\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss: Scalar giving the loss\n",
    "    - dx: Gradient of the loss with respect to x\n",
    "    \"\"\"\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the softmax_loss function.                              #\n",
    "    ###########################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afee8350",
   "metadata": {},
   "source": [
    "You can make sure that the implementations are correct by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca558e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "num_classes, num_inputs = 10, 50\n",
    "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: softmax_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = softmax_loss(x, y)\n",
    "\n",
    "# Test softmax_loss function. Loss should be close to 2.3 and dx error should be around e-8\n",
    "print('\\nTesting softmax_loss:')\n",
    "print('loss: ', loss)\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d4492a",
   "metadata": {},
   "source": [
    "# Loss layer: MSE\n",
    "Now implement the loss and gradient for mean squared error in the `mse_loss` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f5cdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(x, y):\n",
    "    \"\"\"\n",
    "    Computes the loss and gradient for MSE loss.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data, of shape (N,) where x[i] is the predicted vector for \n",
    "        the ith input.\n",
    "    - y: Vector of target values, of shape (N,) where y[i] is the target value\n",
    "        for the ith input.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss: Scalar giving the loss\n",
    "    - dx: Gradient of the loss with respect to x\n",
    "    \"\"\"\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the mse_loss function.                                  #\n",
    "    ###########################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b5b9db",
   "metadata": {},
   "source": [
    "You can make sure that the implementations are correct by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e850132",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "num_inputs = 50\n",
    "x = np.random.randn(num_inputs)\n",
    "y = np.random.randn(num_inputs)\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: mse_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = mse_loss(x, y)\n",
    "\n",
    "# Test mse_loss function. Loss should be close to 1.9 and dx error should be around e-9\n",
    "print('\\nTesting mse_loss:')\n",
    "print('loss: ', loss)\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a7bb75",
   "metadata": {},
   "source": [
    "# Multi-Layer Fully Connected Network\n",
    "In this part, you will implement a fully connected network with an arbitrary number of hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd460d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedNet(object):\n",
    "    \"\"\"Class for a multi-layer fully connected neural network.\n",
    "\n",
    "    Network contains an arbitrary number of hidden layers, ReLU nonlinearities,\n",
    "    and a softmax loss function for a classification problem or the MSE loss function for \n",
    "    a regression problem. This will also implement batch normalization as an option. \n",
    "    For a network with L layers, the architecture will be\n",
    "\n",
    "    {affine - [batchnorm] - relu} x (L - 1) - affine - softmax/mse\n",
    "\n",
    "    where batch normalization is optional in each layer and the {...} block is\n",
    "    repeated L - 1 times.\n",
    "\n",
    "    Learnable parameters are stored in the self.params dictionary and will be learned\n",
    "    using the Solver class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        category,\n",
    "        hidden_dims,\n",
    "        normalization,\n",
    "        input_dim=784,\n",
    "        output_dim=10,\n",
    "        reg=0.0,\n",
    "        weight_scale=1e-2,\n",
    "        dtype=np.float32,\n",
    "    ):\n",
    "        \"\"\"Initialize a new FullyConnectedNet.\n",
    "\n",
    "        Inputs:\n",
    "        - category: The type of the problem. Valid values are \"classification\",\n",
    "            \"regression\".\n",
    "        - hidden_dims: A list of integers giving the size of each hidden layer.\n",
    "        - normalization: A list of booleans which shows that we have batch \n",
    "            normalization after the affine layer.\n",
    "        - input_dim: An integer giving the size of the input.\n",
    "        - output_dim: An integer giving the number of classes to classify. It\n",
    "            is 1 for a regression problem.\n",
    "        - reg: Scalar giving L2 regularization strength.\n",
    "        - weight_scale: Scalar giving the standard deviation for random\n",
    "            initialization of the weights.\n",
    "        - dtype: A numpy datatype object; all computations will be performed using\n",
    "            this datatype. float32 is faster but less accurate, so you should use\n",
    "            float64 for numeric gradient checking.\n",
    "        \"\"\"\n",
    "        self.category = category\n",
    "        self.normalization = normalization\n",
    "        self.reg = reg\n",
    "        self.num_layers = 1 + len(hidden_dims)\n",
    "        self.dtype = dtype\n",
    "        self.params = {}\n",
    "\n",
    "        ############################################################################\n",
    "        # TODO: Initialize the parameters of the network, storing all values in    #\n",
    "        # the self.params dictionary. Store weights and biases for the first layer #\n",
    "        # in W1 and b1; for the second layer use W2 and b2, etc. Weights should be #\n",
    "        # initialized from a normal distribution centered at 0 with standard       #\n",
    "        # deviation equal to weight_scale. Biases should be initialized to zero.   #\n",
    "        #                                                                          #\n",
    "        # When using batch normalization, store scale and shift parameters for the #\n",
    "        # first layer in gamma1 and beta1; for the second layer use gamma2 and     #\n",
    "        # beta2, etc. Scale parameters should be initialized to ones and shift     #\n",
    "        # parameters should be initialized to zeros.                               #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "        pass\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "\n",
    "        # With batch normalization we need to keep track of running means and\n",
    "        # variances, so we need to pass a special bn_param object to each batch\n",
    "        # normalization layer. You should pass self.bn_params[0] to the forward pass\n",
    "        # of the first batch normalization layer, self.bn_params[1] to the forward\n",
    "        # pass of the second batch normalization layer, etc.\n",
    "        self.bn_params = [{\"mode\": \"train\"} for i in range(self.num_layers - 1)]\n",
    "\n",
    "        # Cast all parameters to the correct datatype.\n",
    "        for k, v in self.params.items():\n",
    "            self.params[k] = v.astype(dtype)\n",
    "\n",
    "    def loss(self, X, y=None):\n",
    "        \"\"\"Compute loss and gradient for the fully connected net.\n",
    "        \n",
    "        Inputs:\n",
    "        - X: Array of input data of shape (N, d_1, ..., d_k)\n",
    "        - y: Array of labels / target values, of shape (N,). y[i] gives the \n",
    "            label / target value for X[i].\n",
    "\n",
    "        Returns:\n",
    "        If y is None, then run a test-time forward pass of the model and return\n",
    "        scores for a classification problem or the predicted_values for \n",
    "        a regression problem:\n",
    "        - out: Array of shape (N, C) / (N, ) giving classification scores / predicted values, where \n",
    "        scores[i, c] is the classification score for X[i] and class c / predicted_values[i]\n",
    "        is the predicted value for X[i].\n",
    "            \n",
    "\n",
    "        If y is not None, then run a training-time forward and backward pass and\n",
    "        return a tuple of:\n",
    "        - loss: Scalar value giving the loss\n",
    "        - grads: Dictionary with the same keys as self.params, mapping parameter\n",
    "            names to gradients of the loss with respect to those parameters.\n",
    "        \"\"\"\n",
    "        X = X.astype(self.dtype)\n",
    "        mode = \"test\" if y is None else \"train\"\n",
    "\n",
    "        # Set train/test mode for batchnorm params since they\n",
    "        # behave differently during training and testing.\n",
    "        for bn_param in self.bn_params:\n",
    "            bn_param[\"mode\"] = mode\n",
    "        ############################################################################\n",
    "        # TODO: Implement the forward pass for the fully connected net, computing  #\n",
    "        # the class scores / target values for X and storing them in the out       #\n",
    "        #  variable.                                                               #\n",
    "        #                                                                          #\n",
    "        # When using batch normalization, you'll need to pass self.bn_params[0] to #\n",
    "        # the forward pass for the first batch normalization layer, pass           #\n",
    "        # self.bn_params[1] to the forward pass for the second batch normalization #\n",
    "        # layer, etc.                                                              #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "        pass\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "\n",
    "        # If test mode return early.\n",
    "        if mode == \"test\":\n",
    "            return out\n",
    "            \n",
    "        loss, grads = 0.0, {}\n",
    "        ############################################################################\n",
    "        # TODO: Implement the backward pass for the fully connected net. Store the #\n",
    "        # loss in the loss variable and gradients in the grads dictionary. Compute #\n",
    "        # data loss using softmax/mse, and make sure that grads[k] holds the       #\n",
    "        # gradients for self.params[k]. Don't forget to add L2 regularization!     #\n",
    "        #                                                                          #\n",
    "        # When using batch normalization, you don't need to regularize the scale   #\n",
    "        # and shift parameters.                                                    #\n",
    "        #                                                                          #\n",
    "        # NOTE: To ensure that your implementation matches ours and you pass the   #\n",
    "        # automated tests, make sure that your L2 regularization includes a factor #\n",
    "        # of 0.5 to simplify the expression for the gradient.                      #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        pass\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "\n",
    "        return loss, grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14a2795",
   "metadata": {},
   "source": [
    "## Initial Loss and Gradient Check\n",
    "\n",
    "As a sanity check, run the following to check the initial loss and to gradient check the network both with and without regularization. This is a good way to see if the initial losses seem reasonable.\n",
    "\n",
    "For gradient checking, you should expect to see errors around 1e-7 or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf719033",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "for reg in [0, 3.14]:\n",
    "    print(\"Running check with reg = \", reg)\n",
    "    model = FullyConnectedNet(\n",
    "        \"classification\",\n",
    "        [H1, H2],\n",
    "        [False, False],\n",
    "        input_dim=D,\n",
    "        output_dim=C,\n",
    "        reg=reg,\n",
    "        weight_scale=5e-2,\n",
    "        dtype=np.float64\n",
    "    )\n",
    "\n",
    "    loss, grads = model.loss(X, y)\n",
    "    print(\"Initial loss: \", loss)\n",
    "\n",
    "    # Most of the errors should be on the order of e-7 or smaller.   \n",
    "    # NOTE: It is fine however to see an error for W2 on the order of e-5\n",
    "    # for the check when reg = 0.0\n",
    "    for name in sorted(grads):\n",
    "        f = lambda _: model.loss(X, y)[0]\n",
    "        grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)\n",
    "        print(f\"{name} relative error: {rel_error(grad_num, grads[name])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc48caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "# You should expect losses between 1e-4~1e-10 for W, \n",
    "# losses between 1e-08~1e-10 for b,\n",
    "# and losses between 1e-08~1e-09 for beta and gammas.\n",
    "for reg in [0, 3.14]:\n",
    "    print('Running check with reg = ', reg)\n",
    "    model = FullyConnectedNet(\n",
    "        \"classification\", \n",
    "        [H1, H2],\n",
    "        [True, True],\n",
    "        input_dim=D,\n",
    "        output_dim=C,\n",
    "        reg=reg,\n",
    "        weight_scale=5e-2,\n",
    "        dtype=np.float64\n",
    "    )\n",
    "\n",
    "    loss, grads = model.loss(X, y)\n",
    "    print('Initial loss: ', loss)\n",
    "\n",
    "    for name in sorted(grads):\n",
    "        f = lambda _: model.loss(X, y)[0]\n",
    "        grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)\n",
    "        print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))\n",
    "    if reg == 0: print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6042ddcf",
   "metadata": {},
   "source": [
    "## SGD+Momentum\n",
    "Stochastic gradient descent with momentum is a widely used update rule that tends to make deep networks converge faster than vanilla stochastic gradient descent.\n",
    "\n",
    "Implement the SGD+momentum update rule in the function `sgd_momentum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e678506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_momentum(w, dw, config=None):\n",
    "    \"\"\"\n",
    "    Performs stochastic gradient descent with momentum.\n",
    "    Inputs:\n",
    "    - w: A numpy array giving the current weights.\n",
    "    - dw: A numpy array of the same shape as w giving the gradient of the\n",
    "    loss with respect to w.\n",
    "    - config: A dictionary containing hyperparameter values such as learning\n",
    "    rate, momentum.\n",
    "\n",
    "    Returns:\n",
    "      - next_w: The next point after the update.\n",
    "      - config: The config dictionary to be passed to the next iteration of the\n",
    "        update rule.\n",
    "\n",
    "    config format:\n",
    "    - learning_rate: Scalar learning rate.\n",
    "    - momentum: Scalar between 0 and 1 giving the momentum value.\n",
    "      Setting momentum = 0 reduces sgd_momentum to stochastic gradient descent.\n",
    "    - velocity: A numpy array of the same shape as w and dw used to store a\n",
    "      moving average of the gradients.\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = {}\n",
    "    config.setdefault(\"learning_rate\", 1e-2)\n",
    "    config.setdefault(\"momentum\", 0.9)\n",
    "    v = config.get(\"velocity\", np.zeros_like(w))\n",
    "\n",
    "    next_w = None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the momentum update formula. Store the updated value in #\n",
    "    # the next_w variable. You should also use and update the velocity v.     #\n",
    "    ###########################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    pass\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    config[\"velocity\"] = v\n",
    "\n",
    "    return next_w, config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1730d10",
   "metadata": {},
   "source": [
    "Run the following to check your implementation. You should see errors less than e-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3595fc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {\"learning_rate\": 1e-3, \"velocity\": v}\n",
    "next_w, _ = sgd_momentum(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [ 0.1406,      0.20738947,  0.27417895,  0.34096842,  0.40775789],\n",
    "  [ 0.47454737,  0.54133684,  0.60812632,  0.67491579,  0.74170526],\n",
    "  [ 0.80849474,  0.87528421,  0.94207368,  1.00886316,  1.07565263],\n",
    "  [ 1.14244211,  1.20923158,  1.27602105,  1.34281053,  1.4096    ]])\n",
    "expected_velocity = np.asarray([\n",
    "  [ 0.5406,      0.55475789,  0.56891579, 0.58307368,  0.59723158],\n",
    "  [ 0.61138947,  0.62554737,  0.63970526,  0.65386316,  0.66802105],\n",
    "  [ 0.68217895,  0.69633684,  0.71049474,  0.72465263,  0.73881053],\n",
    "  [ 0.75296842,  0.76712632,  0.78128421,  0.79544211,  0.8096    ]])\n",
    "\n",
    "# Should see relative errors around e-8 or less\n",
    "print(\"next_w error: \", rel_error(next_w, expected_next_w))\n",
    "print(\"velocity error: \", rel_error(expected_velocity, config[\"velocity\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc30c93e",
   "metadata": {},
   "source": [
    "# MNIST\n",
    "MNIST is a widely used dataset of handwritten digits that contains 60,000 handwritten digits for training a machine learning model and 10,000 handwritten digits for testing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fa8400",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = get_MNIST_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e15a728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some examples from the dataset.\n",
    "# We show a few examples of training images from each class.\n",
    "classes = list(range(10))\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(y_train == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(X_train[idx].reshape((28, 28)))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6ad76a",
   "metadata": {},
   "source": [
    "Data normalization is an important step which ensures that each input parameter has a similar data distribution. This makes convergence faster while training the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea87dfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test = get_normalized_MNIST_data(X_train, X_val, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed0af35",
   "metadata": {},
   "source": [
    "# Train a Good Model!\n",
    "Open the file `solver.py` and read through it to familiarize yourself with the API. After doing so, use a `Solver` instance to train the best fully connected model that you can on MNIST, storing your best model in the `MNIST_best_model` variable. We require you to get at least 95% accuracy on the validation set using a fully connected network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d59d748",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_best_model = None\n",
    "\n",
    "################################################################################\n",
    "# TODO: Train the best FullyConnectedNet that you can on MNIST. You might      #\n",
    "# find batch normalization. Store your best model in                           #\n",
    "# the best_model variable.                                                     #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f8a45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to visualize training loss and train / val accuracy\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.plot(MNIST_solver.loss_history, 'o')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(MNIST_solver.train_acc_history, '-o', label='train')\n",
    "plt.plot(MNIST_solver.val_acc_history, '-o', label='val')\n",
    "plt.plot([0.8] * len(MNIST_solver.val_acc_history), 'k--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ff7e18",
   "metadata": {},
   "source": [
    "# Test Your Model!\n",
    "Run your best model on the validation and test sets. You should achieve at least 95% accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df244fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = np.argmax(MNIST_best_model.loss(data['X_test']), axis=1)\n",
    "y_val_pred = np.argmax(MNIST_best_model.loss(data['X_val']), axis=1)\n",
    "print('Validation set accuracy: ', (y_val_pred == data['y_val']).mean())\n",
    "print('Test set accuracy: ', (y_test_pred == data['y_test']).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9b7aa1",
   "metadata": {},
   "source": [
    "# California housing dataset\n",
    "This is a dataset obtained from the [StatLib repository](https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html). The data pertains to the houses found in a given California district and some summary stats about them based on the 1990 census data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ee041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "california_housing = fetch_california_housing(as_frame=True)\n",
    "california_housing.frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc5ad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = get_california_housing_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train target values shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation target values shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test target values shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a1eeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test = get_california_housing_normalized__data(X_train, X_val, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1179b5c",
   "metadata": {},
   "source": [
    "# Train a Good Model!\n",
    "Train the best fully connected model that you can on california housing, storing your best model in the `california_housing_best_model` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fae7eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "california_housing_best_model = None\n",
    "\n",
    "################################################################################\n",
    "# TODO: Train the best FullyConnectedNet that you can on california housing.   #\n",
    "# You might find batch normalization useful. Store your best model in          #\n",
    "# the best_model variable.                                                     #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e51742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to visualize training loss and train / val RMS error\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.plot(california_housing_solver.loss_history, 'o')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('RMS Error')\n",
    "plt.plot(california_housing_solver.train_acc_history, '-o', label='train')\n",
    "plt.plot(california_housing_solver.val_acc_history, '-o', label='val')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
